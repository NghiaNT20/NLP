{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 25. ◑ Define sent to be the list of words ['she', 'sells', 'sea', 'shells', 'by',\n",
    " 'the', 'sea', 'shore']. Now write code to perform the following tasks:\n",
    " a. Print all words beginning with sh.\n",
    " b. Print all words longer than four characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the list of words\n",
    "sent = ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']\n",
    "\n",
    "# a. Print all words beginning with \"sh\"\n",
    "sh_words = [word for word in sent if re.match(r'^sh', word)]\n",
    "print(\"Words beginning with 'sh':\", sh_words)\n",
    "\n",
    "# b. Print all words longer than four characters\n",
    "long_words = [word for word in sent if re.match(r'^.{5,}$', word)]\n",
    "print(\"Words longer than four characters:\", long_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sh']\n",
      "Yes: she\n",
      "No: sells\n",
      "No: sea\n",
      "['sh']\n",
      "Yes: shells\n",
      "No: by\n",
      "No: the\n",
      "No: sea\n",
      "['sh']\n",
      "Yes: shore\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sent = ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']\n",
    "def sh_words(sent):\n",
    "    for word in sent:\n",
    "        result = re.findall(r\"^sh\", word)\n",
    "        if result:\n",
    "            print(result)\n",
    "            print(f\"Yes: {word}\")\n",
    "        else:\n",
    "            print(f\"No: {word}\")\n",
    "print(sh_words(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: she\n",
      "['sells']\n",
      "Yes: sells\n",
      "No: sea\n",
      "['shells']\n",
      "Yes: shells\n",
      "No: by\n",
      "No: the\n",
      "No: sea\n",
      "['shore']\n",
      "Yes: shore\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sent = ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']\n",
    "def long_words(sent):\n",
    "     for word in sent:\n",
    "        result = re.findall(r\"\\b\\w{4,}\\b\", word)\n",
    "        if result:\n",
    "            print(result)\n",
    "            print(f\"Yes: {word}\")\n",
    "        else:\n",
    "            print(f\"No: {word}\")\n",
    "print(long_words(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. ◑ Write expressions for finding all words in text6 that meet the following condi\n",
    "tions. The result should be in the form of a list of words: ['word1', 'word2', ...]\n",
    "a. Ending in ize\n",
    "b. Containing the letter z\n",
    "c. Containing the sequence of letters pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. ending in ize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['organize']\n",
      "Yes: organize\n",
      "No: pendulum\n",
      "No: petunias\n",
      "['realize']\n",
      "Yes: realize\n",
      "['recognize']\n",
      "Yes: recognize\n",
      "No: hostas\n",
      "No: pelorism\n",
      "['visualize']\n",
      "Yes: visualize\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = ['organize', 'pendulum', 'petunias', 'realize', 'recognize', 'hostas', 'pelorism', 'visualize']\n",
    "def end_ize_words(text):\n",
    "    for word in text:\n",
    "        result = re.findall(r\"\\b\\w+ize\\b\", word)\n",
    "        if result:\n",
    "            print(result)\n",
    "            print(f\"Yes: {word}\")\n",
    "        else:\n",
    "            print(f\"No: {word}\")\n",
    "print(end_ize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. containing the letter z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: concept\n",
      "No: accept\n",
      "No: intercept\n",
      "No: adopt\n",
      "['recognize']\n",
      "Yes: recognize\n",
      "No: hostas\n",
      "No: pelorism\n",
      "['visualize']\n",
      "Yes: visualize\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = ['concept', 'accept', 'intercept', 'adopt', 'recognize', 'hostas', 'pelorism', 'visualize']\n",
    "def end_ize_words(text):\n",
    "    for word in text:\n",
    "        result = re.findall(r\"\\b\\w*z\\w*\\b\", word)\n",
    "        if result:\n",
    "            print(result)\n",
    "            print(f\"Yes: {word}\")\n",
    "        else:\n",
    "            print(f\"No: {word}\")\n",
    "print(end_ize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Finding all words containing the sequence 'pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['concept']\n",
      "Yes: concept\n",
      "['accept']\n",
      "Yes: accept\n",
      "['intercept']\n",
      "Yes: intercept\n",
      "['adopt']\n",
      "Yes: adopt\n",
      "['recoptgnize']\n",
      "Yes: recoptgnize\n",
      "No: hostas\n",
      "No: pelorism\n",
      "No: visualize\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = ['concept', 'accept', 'intercept', 'adopt', 'recognize', 'hostas', 'pelorism', 'visualize']\n",
    "def end_ize_words(text):\n",
    "    for word in text:\n",
    "        result = re.findall(r\"\\b\\w*pt\\w*\\b\", word)\n",
    "        if result:\n",
    "            print(result)\n",
    "            print(f\"Yes: {word}\")\n",
    "        else:\n",
    "            print(f\"No: {word}\")\n",
    "print(end_ize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 21. ◑ Write the slice expression that extracts the last two words of text2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She received the news of the discovery with equanimty\n",
      "['with', 'equanimty']\n"
     ]
    }
   ],
   "source": [
    "# tự cho chuỗi kí tụư\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = 'She received the news of the discovery with equanimty'\n",
    "print(text)\n",
    "print(text.split()[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She received the news of the discovery with equanimty\n",
      "['She', 'received']\n"
     ]
    }
   ],
   "source": [
    "# cho trường hợp 2 từ ban đầu\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = 'She received the news of the discovery with equanimty'\n",
    "print(text)\n",
    "print(text.split()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "Last two words of text2: ['THE', 'END']\n"
     ]
    }
   ],
   "source": [
    "#cho trường hợp 2 từ cuối của text2\n",
    "from nltk.book import text2\n",
    "last_two_words = text2[-2:]\n",
    "print(\"Last two words of text2:\", last_two_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. ◑ Find all the four-letter words in the Chat Corpus (text5). With the help of a\n",
    " frequency distribution (FreqDist), show these words in decreasing order of fre\n",
    "quency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four-Letter Words in Chat Corpus (text5) in Decreasing Order of Frequency:\n",
      "part: 1022\n",
      "join: 1021\n",
      "that: 284\n",
      "what: 201\n",
      "here: 185\n",
      "have: 171\n",
      "like: 160\n",
      "with: 154\n",
      "chat: 146\n",
      "your: 142\n",
      "good: 132\n",
      "lmao: 128\n",
      "just: 128\n",
      "know: 104\n",
      "room: 103\n",
      "this: 98\n",
      "from: 96\n",
      "well: 91\n",
      "yeah: 85\n",
      "hiya: 85\n",
      "they: 84\n",
      "back: 79\n",
      "dont: 77\n",
      "want: 71\n",
      "love: 63\n",
      "guys: 59\n",
      "some: 59\n",
      "talk: 58\n",
      "been: 58\n",
      "nice: 54\n",
      "when: 54\n",
      "time: 52\n",
      "haha: 46\n",
      "girl: 45\n",
      "make: 44\n",
      "need: 44\n",
      "will: 42\n",
      "song: 42\n",
      "mode: 42\n",
      "then: 41\n",
      "much: 40\n",
      "over: 40\n",
      "were: 39\n",
      "take: 38\n",
      "does: 38\n",
      "work: 38\n",
      "even: 37\n",
      "only: 36\n",
      "damn: 36\n",
      "seen: 36\n",
      "come: 36\n",
      "more: 35\n",
      "nick: 33\n",
      "long: 30\n",
      "hell: 29\n",
      "last: 29\n",
      "tell: 29\n",
      "name: 28\n",
      "away: 28\n",
      "sure: 28\n",
      "them: 28\n",
      "look: 27\n",
      "baby: 26\n",
      "call: 26\n",
      "down: 26\n",
      "cool: 26\n",
      "sexy: 25\n",
      "play: 25\n",
      "many: 23\n",
      "hate: 23\n",
      "said: 23\n",
      "same: 23\n",
      "live: 23\n",
      "stop: 23\n",
      "life: 22\n",
      "ever: 22\n",
      "hear: 21\n",
      "very: 20\n",
      "feel: 19\n",
      "hugs: 19\n",
      "mean: 19\n",
      "give: 19\n",
      "must: 19\n",
      "find: 18\n",
      "cant: 18\n",
      "left: 17\n",
      "fine: 17\n",
      "lets: 17\n",
      "shit: 17\n",
      "nite: 17\n",
      "busy: 17\n",
      "hair: 17\n",
      "lost: 17\n",
      "eyes: 16\n",
      "real: 16\n",
      "heya: 16\n",
      "game: 16\n",
      "fuck: 15\n",
      "sits: 15\n",
      "kill: 15\n",
      "read: 14\n",
      "else: 14\n",
      "shut: 14\n",
      "wait: 14\n",
      "goes: 14\n",
      "keep: 14\n",
      "nope: 14\n",
      "true: 14\n",
      "male: 13\n",
      "pick: 13\n",
      "free: 13\n",
      "near: 13\n",
      "awww: 13\n",
      "hope: 12\n",
      "head: 12\n",
      "than: 12\n",
      "kids: 12\n",
      "face: 12\n",
      "gets: 12\n",
      "cold: 12\n",
      "hehe: 12\n",
      "bout: 12\n",
      "home: 12\n",
      "stay: 12\n",
      "used: 12\n",
      "told: 12\n",
      "yall: 12\n",
      "days: 11\n",
      "hard: 11\n",
      "show: 11\n",
      "doin: 11\n",
      "perv: 11\n",
      "wont: 11\n",
      "dang: 11\n",
      "year: 11\n",
      "babe: 11\n",
      "help: 11\n",
      "into: 11\n",
      "type: 10\n",
      "heyy: 10\n",
      "rock: 10\n",
      "mine: 10\n",
      "once: 10\n",
      "care: 10\n",
      "mind: 10\n",
      "kiss: 10\n",
      "week: 10\n",
      "liam: 10\n",
      "pics: 9\n",
      "such: 9\n",
      "best: 9\n",
      "neck: 9\n",
      "dead: 9\n",
      "okay: 9\n",
      "runs: 9\n",
      "aint: 9\n",
      "book: 9\n",
      "crap: 9\n",
      "soon: 9\n",
      "full: 9\n",
      "hour: 9\n",
      "ahhh: 9\n",
      "sick: 9\n",
      "hmmm: 9\n",
      "hand: 8\n",
      "word: 8\n",
      "rule: 8\n",
      "poor: 8\n",
      "dude: 8\n",
      "case: 8\n",
      "wana: 8\n",
      "hows: 8\n",
      "went: 8\n",
      "lady: 8\n",
      "oops: 8\n",
      "blue: 8\n",
      "kick: 8\n",
      "says: 8\n",
      "suck: 8\n",
      "made: 8\n",
      "wife: 8\n",
      "sang: 8\n",
      "fast: 7\n",
      "alot: 7\n",
      "none: 7\n",
      "took: 7\n",
      "wear: 7\n",
      "kool: 7\n",
      "dear: 7\n",
      "food: 7\n",
      "late: 6\n",
      "most: 6\n",
      "ummm: 6\n",
      "thru: 6\n",
      "caps: 6\n",
      "list: 6\n",
      "came: 6\n",
      "seem: 6\n",
      "cali: 6\n",
      "sing: 6\n",
      "next: 6\n",
      "done: 6\n",
      "ride: 6\n",
      "comp: 6\n",
      "high: 6\n",
      "main: 6\n",
      "goin: 6\n",
      "pink: 6\n",
      "gone: 6\n",
      "ohhh: 6\n",
      "knew: 6\n",
      "ball: 6\n",
      "send: 6\n",
      "woot: 6\n",
      "blah: 6\n",
      "whos: 6\n",
      "yoko: 6\n",
      "sock: 6\n",
      "legs: 5\n",
      "fire: 5\n",
      "warm: 5\n",
      "turn: 5\n",
      "hang: 5\n",
      "miss: 5\n",
      "boys: 5\n",
      "land: 5\n",
      "huge: 5\n",
      "nose: 5\n",
      "lick: 5\n",
      "wish: 5\n",
      "quit: 5\n",
      "roll: 5\n",
      "easy: 5\n",
      "lose: 5\n",
      "cute: 5\n",
      "holy: 5\n",
      "soul: 5\n",
      "luck: 5\n",
      "also: 5\n",
      "evil: 5\n",
      "fall: 5\n",
      "lord: 5\n",
      "boss: 5\n",
      "beer: 5\n",
      "wall: 5\n",
      "meet: 5\n",
      "till: 5\n",
      "feet: 5\n",
      "xbox: 5\n",
      "idea: 5\n",
      "heck: 5\n",
      "joke: 5\n",
      "fool: 5\n",
      "felt: 5\n",
      "kent: 5\n",
      "meds: 5\n",
      "both: 5\n",
      "lime: 5\n",
      "drew: 4\n",
      "glad: 4\n",
      "jerk: 4\n",
      "ugly: 4\n",
      "date: 4\n",
      "lies: 4\n",
      "rest: 4\n",
      "door: 4\n",
      "self: 4\n",
      "pass: 4\n",
      "line: 4\n",
      "hook: 4\n",
      "ohio: 4\n",
      "each: 4\n",
      "swim: 4\n",
      "open: 4\n",
      "ouch: 4\n",
      "fart: 4\n",
      "grrr: 4\n",
      "pain: 4\n",
      "hawt: 4\n",
      "wind: 4\n",
      "pfft: 4\n",
      "sigh: 4\n",
      "rofl: 4\n",
      "shes: 4\n",
      "mary: 4\n",
      "mmmm: 4\n",
      "ones: 4\n",
      "shot: 4\n",
      "team: 4\n",
      "hott: 4\n",
      "ways: 4\n",
      "road: 4\n",
      "beat: 4\n",
      "hail: 4\n",
      "elle: 4\n",
      "lame: 4\n",
      "john: 4\n",
      "puff: 4\n",
      "tisk: 4\n",
      "clap: 3\n",
      "itch: 3\n",
      "guyz: 3\n",
      "gold: 3\n",
      "ring: 3\n",
      "isnt: 3\n",
      "deep: 3\n",
      "phil: 3\n",
      "deal: 3\n",
      "wash: 3\n",
      "piff: 3\n",
      "jump: 3\n",
      "whoa: 3\n",
      "gosh: 3\n",
      "band: 3\n",
      "orgy: 3\n",
      "slap: 3\n",
      "hold: 3\n",
      "soft: 3\n",
      "bend: 3\n",
      "toss: 3\n",
      "amen: 3\n",
      "rain: 3\n",
      "deop: 3\n",
      "roof: 3\n",
      "ahem: 3\n",
      "hola: 3\n",
      "drop: 3\n",
      "butt: 3\n",
      "imma: 3\n",
      "town: 3\n",
      "elev: 3\n",
      "akdt: 3\n",
      "lead: 3\n",
      "ding: 3\n",
      "note: 3\n",
      "gawd: 3\n",
      "half: 3\n",
      "ello: 3\n",
      "hick: 3\n",
      "city: 3\n",
      "wine: 3\n",
      "hiii: 3\n",
      "bare: 3\n",
      "vote: 3\n",
      "wack: 3\n",
      "snow: 3\n",
      "hurt: 3\n",
      "born: 3\n",
      "move: 3\n",
      "walk: 3\n",
      "yawn: 3\n",
      "nana: 3\n",
      "hump: 3\n",
      "yada: 3\n",
      "kewl: 3\n",
      "tune: 3\n",
      "hank: 3\n",
      "slow: 3\n",
      "rubs: 3\n",
      "skin: 3\n",
      "died: 3\n",
      "army: 3\n",
      "wazz: 3\n",
      "toes: 3\n",
      "golf: 2\n",
      "cast: 2\n",
      "slip: 2\n",
      "opps: 2\n",
      "plan: 2\n",
      "deaf: 2\n",
      "hmph: 2\n",
      "bite: 2\n",
      "mins: 2\n",
      "eats: 2\n",
      "cell: 2\n",
      "cmon: 2\n",
      "wats: 2\n",
      "kind: 2\n",
      "mike: 2\n",
      "dumb: 2\n",
      "park: 2\n",
      "loud: 2\n",
      "mama: 2\n",
      "whip: 2\n",
      "pour: 2\n",
      "twin: 2\n",
      "burp: 2\n",
      "kold: 2\n",
      "blew: 2\n",
      "temp: 2\n",
      "corn: 2\n",
      "pool: 2\n",
      "cash: 2\n",
      "ears: 2\n",
      "porn: 2\n",
      "heal: 2\n",
      "ciao: 2\n",
      "typo: 2\n",
      "halo: 2\n",
      "eric: 2\n",
      "sore: 2\n",
      "size: 2\n",
      "hits: 2\n",
      "past: 2\n",
      "meat: 2\n",
      "argh: 2\n",
      "limp: 2\n",
      "rent: 2\n",
      "cars: 2\n",
      "shop: 2\n",
      "five: 2\n",
      "sell: 2\n",
      "yard: 2\n",
      "grrl: 2\n",
      "chip: 2\n",
      "bear: 2\n",
      "foot: 2\n",
      "uses: 2\n",
      "sort: 2\n",
      "hill: 2\n",
      "mono: 2\n",
      "whud: 2\n",
      "bone: 2\n",
      "club: 2\n",
      "adds: 2\n",
      "area: 2\n",
      "humm: 2\n",
      "newp: 2\n",
      "gays: 2\n",
      "pmsl: 2\n",
      "zone: 2\n",
      "hint: 2\n",
      "spin: 2\n",
      "ewww: 2\n",
      "pies: 2\n",
      "doll: 2\n",
      "gimp: 2\n",
      "spot: 2\n",
      "ages: 2\n",
      "clue: 2\n",
      "mass: 2\n",
      "flow: 2\n",
      "west: 2\n",
      "rang: 2\n",
      "hall: 2\n",
      "haze: 2\n",
      "seee: 2\n",
      "sooo: 2\n",
      "cost: 2\n",
      "trip: 2\n",
      "babi: 2\n",
      "rich: 2\n",
      "teck: 2\n",
      "moon: 2\n",
      "yeas: 2\n",
      "wooo: 2\n",
      "tick: 2\n",
      "tock: 2\n",
      "rush: 2\n",
      "side: 2\n",
      "howz: 2\n",
      "root: 2\n",
      "tyvm: 2\n",
      "luvs: 2\n",
      "fits: 2\n",
      "sand: 2\n",
      "king: 2\n",
      "ltns: 2\n",
      "flaw: 2\n",
      "aunt: 2\n",
      "lawl: 2\n",
      "tiff: 2\n",
      "draw: 1\n",
      "docs: 1\n",
      "fade: 1\n",
      "bowl: 1\n",
      "bong: 1\n",
      "ogan: 1\n",
      "cams: 1\n",
      "gooo: 1\n",
      "yeee: 1\n",
      "ahah: 1\n",
      "jeep: 1\n",
      "vbox: 1\n",
      "serg: 1\n",
      "bein: 1\n",
      "whys: 1\n",
      "tape: 1\n",
      "sexs: 1\n",
      "form: 1\n",
      "nads: 1\n",
      "owww: 1\n",
      "gags: 1\n",
      "meep: 1\n",
      "lool: 1\n",
      "kina: 1\n",
      "sext: 1\n",
      "lazy: 1\n",
      "calm: 1\n",
      "arms: 1\n",
      "smax: 1\n",
      "vvil: 1\n",
      "este: 1\n",
      "chik: 1\n",
      "boyz: 1\n",
      "coat: 1\n",
      "dawn: 1\n",
      "mauh: 1\n",
      "ques: 1\n",
      "ruff: 1\n",
      "mame: 1\n",
      "nada: 1\n",
      "push: 1\n",
      "prob: 1\n",
      "wild: 1\n",
      "whew: 1\n",
      "dark: 1\n",
      "waht: 1\n",
      "test: 1\n",
      "boot: 1\n",
      "hiom: 1\n",
      "dman: 1\n",
      "jail: 1\n",
      "cops: 1\n",
      "hogs: 1\n",
      "peek: 1\n",
      "ctrl: 1\n",
      "hots: 1\n",
      "frst: 1\n",
      "crop: 1\n",
      "bomb: 1\n",
      "eeek: 1\n",
      "tjhe: 1\n",
      "heee: 1\n",
      "peel: 1\n",
      "fock: 1\n",
      "exit: 1\n",
      "mris: 1\n",
      "buff: 1\n",
      "plus: 1\n",
      "tory: 1\n",
      "knee: 1\n",
      "oooh: 1\n",
      "lala: 1\n",
      "fake: 1\n",
      "ssid: 1\n",
      "poot: 1\n",
      "poop: 1\n",
      "bird: 1\n",
      "plow: 1\n",
      "thnx: 1\n",
      "card: 1\n",
      "uyes: 1\n",
      "benz: 1\n",
      "disc: 1\n",
      "bloe: 1\n",
      "blow: 1\n",
      "hooo: 1\n",
      "thje: 1\n",
      "jess: 1\n",
      "term: 1\n",
      "tina: 1\n",
      "ooer: 1\n",
      "anal: 1\n",
      "dojn: 1\n",
      "wubs: 1\n",
      "mkay: 1\n",
      "spat: 1\n",
      "gees: 1\n",
      "puts: 1\n",
      "fish: 1\n",
      "syck: 1\n",
      "tere: 1\n",
      "sent: 1\n",
      "werd: 1\n",
      "nawt: 1\n",
      "sign: 1\n",
      "woof: 1\n",
      "ghet: 1\n",
      "brad: 1\n",
      "offa: 1\n",
      "dood: 1\n",
      "sink: 1\n",
      "cums: 1\n",
      "loss: 1\n",
      "wrap: 1\n",
      "hide: 1\n",
      "okey: 1\n",
      "worl: 1\n",
      "cepn: 1\n",
      "lots: 1\n",
      "nawp: 1\n",
      "addy: 1\n",
      "lake: 1\n",
      "mite: 1\n",
      "wood: 1\n",
      "orta: 1\n",
      "wins: 1\n",
      "ebay: 1\n",
      "coem: 1\n",
      "giva: 1\n",
      "ally: 1\n",
      "judy: 1\n",
      "cyas: 1\n",
      "shup: 1\n",
      "tooo: 1\n",
      "choc: 1\n",
      "wher: 1\n",
      "whoo: 1\n",
      "dint: 1\n",
      "tend: 1\n",
      "menu: 1\n",
      "lust: 1\n",
      "nods: 1\n",
      "kept: 1\n",
      "scuk: 1\n",
      "raed: 1\n",
      "bugs: 1\n",
      "nerd: 1\n",
      "pimp: 1\n",
      "haaa: 1\n",
      "hero: 1\n",
      "joey: 1\n",
      "jane: 1\n",
      "span: 1\n",
      "wore: 1\n",
      "pasa: 1\n",
      "barn: 1\n",
      "feat: 1\n",
      "dork: 1\n",
      "laid: 1\n",
      "herd: 1\n",
      "tide: 1\n",
      "jush: 1\n",
      "grlz: 1\n",
      "lung: 1\n",
      "lion: 1\n",
      "brat: 1\n",
      "muah: 1\n",
      "fawk: 1\n",
      "dust: 1\n",
      "seth: 1\n",
      "abou: 1\n",
      "tthe: 1\n",
      "pork: 1\n",
      "mark: 1\n",
      "dotn: 1\n",
      "gift: 1\n",
      "outs: 1\n",
      "paul: 1\n",
      "outa: 1\n",
      "york: 1\n",
      "fear: 1\n",
      "dies: 1\n",
      "givs: 1\n",
      "bust: 1\n",
      "xmas: 1\n",
      "enuf: 1\n",
      "eeww: 1\n",
      "dick: 1\n",
      "fair: 1\n",
      "lyin: 1\n",
      "lois: 1\n",
      "cuss: 1\n",
      "rape: 1\n",
      "geez: 1\n",
      "tart: 1\n",
      "hgey: 1\n",
      "caan: 1\n",
      "nude: 1\n",
      "allo: 1\n",
      "yesh: 1\n",
      "reub: 1\n",
      "heat: 1\n",
      "kmph: 1\n",
      "pope: 1\n",
      "yess: 1\n",
      "duet: 1\n",
      "wuts: 1\n",
      "quiz: 1\n",
      "scar: 1\n",
      "pair: 1\n",
      "bell: 1\n",
      "dawg: 1\n",
      "febe: 1\n",
      "prof: 1\n",
      "jude: 1\n",
      "whou: 1\n",
      "idnt: 1\n",
      "perk: 1\n",
      "http: 1\n",
      "yell: 1\n",
      "mang: 1\n",
      "ssri: 1\n",
      "cure: 1\n",
      "wean: 1\n",
      "post: 1\n",
      "anti: 1\n",
      "noth: 1\n",
      "tall: 1\n",
      "pray: 1\n",
      "weed: 1\n",
      "icky: 1\n",
      "rick: 1\n",
      "spit: 1\n",
      "lube: 1\n",
      "mami: 1\n",
      "east: 1\n",
      "seat: 1\n",
      "cock: 1\n",
      "otay: 1\n",
      "firs: 1\n",
      "site: 1\n",
      "dump: 1\n",
      "toop: 1\n",
      "four: 1\n",
      "sets: 1\n",
      "asss: 1\n",
      "paid: 1\n",
      "iowa: 1\n",
      "jeff: 1\n",
      "crib: 1\n",
      "drug: 1\n",
      "cook: 1\n",
      "ladz: 1\n",
      "aime: 1\n",
      "hong: 1\n",
      "kong: 1\n",
      "tits: 1\n",
      "gret: 1\n",
      "guns: 1\n",
      "inch: 1\n",
      "sean: 1\n",
      "howl: 1\n",
      "slam: 1\n",
      "pine: 1\n",
      "puke: 1\n",
      "waaa: 1\n",
      "urls: 1\n",
      "star: 1\n",
      "save: 1\n",
      "sori: 1\n",
      "poem: 1\n",
      "jack: 1\n",
      "junk: 1\n",
      "tips: 1\n",
      "nooo: 1\n",
      "troy: 1\n",
      "tail: 1\n",
      "dyed: 1\n",
      "beam: 1\n",
      "daft: 1\n",
      "twit: 1\n",
      "scum: 1\n",
      "toke: 1\n",
      "ribs: 1\n",
      "eggs: 1\n",
      "wyte: 1\n",
      "moms: 1\n",
      "goof: 1\n",
      "able: 1\n",
      "vamp: 1\n",
      "ther: 1\n",
      "text: 1\n",
      "gear: 1\n",
      "matt: 1\n",
      "ntmn: 1\n",
      "grea: 1\n",
      "guts: 1\n",
      "wrek: 1\n",
      "fort: 1\n",
      "akst: 1\n",
      "wire: 1\n",
      "soda: 1\n",
      "gray: 1\n",
      "tlak: 1\n",
      "ltnc: 1\n",
      "sayn: 1\n",
      "evah: 1\n",
      "bike: 1\n",
      "ohwa: 1\n",
      "caca: 1\n",
      "prep: 1\n",
      "pull: 1\n",
      "dirt: 1\n",
      "vent: 1\n",
      "safe: 1\n",
      "dogs: 1\n",
      "bull: 1\n",
      "asks: 1\n",
      "chit: 1\n",
      "grin: 1\n",
      "bred: 1\n",
      "rats: 1\n",
      "samn: 1\n",
      "nuff: 1\n",
      "rose: 1\n",
      "ruth: 1\n",
      "grew: 1\n",
      "mena: 1\n",
      "lapd: 1\n",
      "surf: 1\n",
      "hazy: 1\n",
      "thot: 1\n",
      "acid: 1\n",
      "wide: 1\n",
      "keys: 1\n",
      "salt: 1\n",
      "mess: 1\n",
      "base: 1\n",
      "byes: 1\n",
      "yout: 1\n",
      "numb: 1\n",
      "thah: 1\n",
      "mahn: 1\n",
      "soup: 1\n",
      "vega: 1\n",
      "pigs: 1\n",
      "poof: 1\n",
      "nova: 1\n",
      "mofo: 1\n",
      "sips: 1\n",
      "clay: 1\n",
      "bacl: 1\n",
      "body: 1\n",
      "akon: 1\n",
      "yoll: 1\n",
      "boom: 1\n",
      "news: 1\n",
      "maps: 1\n",
      "page: 1\n",
      "chop: 1\n",
      "typr: 1\n",
      "poll: 1\n",
      "boed: 1\n",
      "pwns: 1\n",
      "sexi: 1\n",
      "bois: 1\n",
      "bied: 1\n",
      "uhhh: 1\n",
      "tenn: 1\n",
      "pure: 1\n",
      "gals: 1\n",
      "woah: 1\n",
      "ussy: 1\n",
      "heys: 1\n",
      "lisa: 1\n",
      "brwn: 1\n",
      "hurr: 1\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import nps_chat\n",
    "\n",
    "# Load the Chat Corpus\n",
    "chat_corpus = nps_chat.words()\n",
    "\n",
    "# Filter out four-letter words\n",
    "four_letter_words = [word.lower() for word in chat_corpus if len(word) == 4 and word.isalpha()]\n",
    "\n",
    "# Create a frequency distribution of four-letter words\n",
    "fdist = FreqDist(four_letter_words)\n",
    "\n",
    "# Show the four-letter words in decreasing order of frequency\n",
    "print(\"Four-Letter Words in Chat Corpus (text5) in Decreasing Order of Frequency:\")\n",
    "for word, frequency in fdist.most_common():\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 27. ◑ Define a function called vocab_size(text) that has a single parameter for the\n",
    " text, and which returns the vocabulary size of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of text2: 6833\n"
     ]
    }
   ],
   "source": [
    "def vocab_size(text):\n",
    "    \"\"\"\n",
    "    Returns the vocabulary size of the given text.\n",
    "    \n",
    "    Parameters:\n",
    "    text (list of str): A list of words representing the text.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of unique words in the text.\n",
    "    \"\"\"\n",
    "    # Convert the text to a set to get unique words\n",
    "    unique_words = set(text)\n",
    "    \n",
    "    # Return the size of the set, which is the number of unique words\n",
    "    return len(unique_words)\n",
    "\n",
    "# Example usage with text2 from the NLTK corpus\n",
    "from nltk.book import text2\n",
    "\n",
    "print(\"Vocabulary size of text2:\", vocab_size(text2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 28. ◑ Define a function percent(word, text) that calculates how often a given word\n",
    " occurs in a text and expresses the result as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'FPT' occurs in the text with a frequency of 20.00%.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def percent(word, text):\n",
    "    words = word_tokenize(text)\n",
    "    word_frequency = words.count(word)\n",
    "    total_words = len(words)\n",
    "    percentage = (word_frequency / total_words) * 100\n",
    "    return percentage\n",
    "text = \"Dai Hoc FPT Quy Nhon\"\n",
    "word = \"FPT\"\n",
    "result = percent(word, text)\n",
    "print(f\"The word '{word}' occurs in the text with a frequency of {result:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'FPT' occurs 20.00% of the time in the given text.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def percent(word, text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Count the occurrences of the given word\n",
    "    word_count = sum(1 for w in words if w.lower() == word.lower())\n",
    "\n",
    "    # Calculate the percentage\n",
    "    percentage = (word_count / len(words)) * 100\n",
    "\n",
    "    return percentage\n",
    "\n",
    "# Example usage\n",
    "text = \"Dai Hoc FPT Quy Nhon\"\n",
    "word = \"FPT\"\n",
    "result = percent(word, text)\n",
    "\n",
    "print(f\"The word '{word}' occurs {result:.2f}% of the time in the given text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 29. ◑ We have been using sets to store vocabularies. Try the following Python expres\n",
    "sion: set(sent3) < set(text1). Experiment with this using different arguments to\n",
    " set(). What does it do? Can you think of a practical application for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is text3 a strict subset of text1? False\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text1, text3\n",
    "is_subset = set(text3) < set(text1)\n",
    "print(\"Is text3 a strict subset of text1?\", is_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. ◑ Using list addition, and the set and sorted operations, compute the vocabulary\n",
    " of the sentences sent1 ... sent8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of sentences sent1 to sent8: ['!', ',', '-', '.', '1', '25', '29', '61', ':', 'ARTHUR', 'Call', 'Citizens', 'Dashwood', 'Fellow', 'God', 'House', 'I', 'In', 'Ishmael', 'JOIN', 'KING', 'MALE', 'Nov.', 'PMing', 'Pierre', 'Representatives', 'SCENE', 'SEXY', 'Senate', 'Sussex', 'The', 'Vinken', 'Whoa', '[', ']', 'a', 'and', 'as', 'attrac', 'been', 'beginning', 'board', 'clop', 'created', 'director', 'discreet', 'earth', 'encounters', 'family', 'for', 'had', 'have', 'heaven', 'in', 'join', 'lady', 'lol', 'long', 'me', 'nonexecutive', 'of', 'old', 'older', 'people', 'problem', 'seeks', 'settled', 'single', 'the', 'there', 'to', 'will', 'wind', 'with', 'years']\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import sent1, sent2, sent3, sent4, sent5, sent6, sent7, sent8\n",
    "\n",
    "# Combine all sentences into one list\n",
    "combined_sentences = sent1 + sent2 + sent3 + sent4 + sent5 + sent6 + sent7 + sent8\n",
    "\n",
    "# Convert to a set to get unique words\n",
    "unique_words = set(combined_sentences)\n",
    "\n",
    "# Sort the unique words\n",
    "sorted_unique_words = sorted(unique_words)\n",
    "\n",
    "print(\"Vocabulary of sentences sent1 to sent8:\", sorted_unique_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. ◑ Use text9.index() to find the index of the word sunset. You’ll need to insert this\n",
    " word as an argument between the parentheses. By a process of trial and error, find\n",
    " the slice for the complete sentence that contains this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 'sunset': 629\n",
      "Sentence containing 'sunset': ['.', 'CHAPTER', 'I', 'THE', 'TWO', 'POETS', 'OF', 'SAFFRON', 'PARK', 'THE', 'suburb', 'of', 'Saffron', 'Park', 'lay', 'on', 'the', 'sunset', 'side', 'of', 'London', ',', 'as', 'red', 'and', 'ragged', 'as', 'a', 'cloud', 'of', 'sunset', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text9\n",
    "\n",
    "# Find the index of the word 'sunset'\n",
    "index_sunset = text9.index('sunset')\n",
    "\n",
    "# Output the index of the word 'sunset'\n",
    "print(\"Index of 'sunset':\", index_sunset)\n",
    "\n",
    "# Find the complete sentence containing 'sunset'\n",
    "# Locate sentence boundaries\n",
    "# Find the start of the sentence by moving backward to the previous period\n",
    "start_index = index_sunset\n",
    "while start_index > 0 and text9[start_index] not in ['.', '!', '?']:\n",
    "    start_index -= 1\n",
    "\n",
    "# Move forward to the end of the sentence\n",
    "end_index = index_sunset\n",
    "while end_index < len(text9) and text9[end_index] not in ['.', '!', '?']:\n",
    "    end_index += 1\n",
    "\n",
    "# Adjust to include the punctuation in the sentence\n",
    "if end_index < len(text9):\n",
    "    end_index += 1\n",
    "\n",
    "# Extract and print the sentence containing 'sunset'\n",
    "sentence_with_sunset = text9[start_index:end_index]\n",
    "print(\"Sentence containing 'sunset':\", sentence_with_sunset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Câu này chương 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ○ Write regular expressions to match the following classes of strings:\n",
    " a. A single determiner (assume that a, an, and the are the only determiners)\n",
    " b. An arithmetic expression using integers, addition, and multiplication, such as\n",
    " 2*3+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'4*5+3' is Match\n",
      "'-4+5*3' is Match\n",
      "'10+50*-2' is Match\n",
      "'-1++2*3' is Match\n",
      "'11+33*33' is Match\n",
      "'5*4+a' is Not Match\n",
      "'4**4+3' is Not Match\n",
      "'4+3*' is Not Match\n",
      "'4+3' is Not Match\n",
      "'2*3+8' is Match\n",
      "'5-2+3*4' is Not Match\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_valid_expression(expression):\n",
    "    # Define the patterns based on the teacher's suggestions\n",
    "    integer_pattern = r'[-+]?\\d+'\n",
    "    addition_multiplication_integer_pattern = r'([\\+\\*][-+]?\\d+)+'\n",
    "    check_condition_pattern = r'(?=.*\\+)(?=.*\\*)'\n",
    "    \n",
    "    # Combine the patterns into the final regex\n",
    "    pattern = (\n",
    "        fr'^{check_condition_pattern}'  # Check if + and * are present\n",
    "        fr'{integer_pattern}'           # Match the initial integer\n",
    "        fr'{addition_multiplication_integer_pattern}'  # Match subsequent additions or multiplications\n",
    "        r'$'                            # End of the string\n",
    "    )\n",
    "    \n",
    "    return re.match(pattern, expression) is not None\n",
    "\n",
    "number = [\n",
    "    \"4*5+3\",\n",
    "    \"-4+5*3\",\n",
    "    \"10+50*-2\",\n",
    "    \"-1++2*3\",\n",
    "    \"11+33*33\",\n",
    "    \"5*4+a\",\n",
    "    \"4**4+3\",\n",
    "    \"4+3*\",\n",
    "    \"4+3\",\n",
    "    \"2*3+8\",\n",
    "    \"5-2+3*4\"\n",
    "]\n",
    "\n",
    "# Test the expressions\n",
    "for exp in number:\n",
    "    print(f\"'{exp}' is {'Match' if is_valid_expression(exp) else 'Not Match'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n",
      "Match: a\n",
      "['an']\n",
      "Match: an\n",
      "['the']\n",
      "Match: the\n",
      "['a']\n",
      "Match:  a\n",
      "['an']\n",
      "Match:   an \n",
      "Not match: A\n",
      "Not match:  The\n",
      "Not match: a person\n",
      "Not match: an a the\n",
      "Not match: aa\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = ['a',\n",
    "             'an',\n",
    "             'the',\n",
    "             ' a',\n",
    "             '  an ',\n",
    "             'A',\n",
    "             ' The',\n",
    "             'a person',\n",
    "             'an a the',\n",
    "             'aa']\n",
    "def single_determiner(text):\n",
    "    for word in text:\n",
    "        result = re.findall(r\"^\\s*(a|an|the)\\s*$\", word)\n",
    "        if result:\n",
    "            print(result)\n",
    "            print(f\"Match: {word}\")\n",
    "        else:\n",
    "            print(f\"Not match: {word}\")\n",
    "print(single_determiner(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
